{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ed7b5e",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Engineering & Training Windows\n",
    "\n",
    "This notebook:\n",
    "1. **Loads data** from Phase 1 (with entry-based heatmaps)\n",
    "2. **Engineers features** from existing FeaturesLibrary\n",
    "3. **Creates sliding windows** for sequence modeling\n",
    "4. **Prepares train/test splits** with proper time-series validation\n",
    "\n",
    "Goal: Transform raw price data + heatmap labels into ML-ready sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b34d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add FEATURES directory to path for FeaturesLibrary\n",
    "sys.path.append('../FEATURES')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460e26e",
   "metadata": {},
   "source": [
    "## 1. Load Data from Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cdd7be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (13676, 37)\n",
      "Date range: 2015-01-02 12:00:00 to 2023-09-20 08:00:00\n",
      "\n",
      "Total events: 327\n",
      "\n",
      "Available columns: ['open', 'high', 'low', 'close', 'tick_volume', 'low_time', 'high_time', 'ma_fast', 'ma_slow', 'signal', 'signal_shift', 'entry_long', 'entry_short', 'entry_signal', 'outcome_label', 'in_trade', 'trade_id', 'tp_heatmap', 'sl_heatmap', 'combined_heatmap', 'tp_prob', 'sl_prob', 'nohit_prob', 'tp_prob_raw', 'sl_prob_raw', 'signed_heatmap', 'tp_signed', 'sl_signed', 'heatmap_magnitude', 'heatmap_direction', 'tp_heatmap_only', 'sl_heatmap_only', 'tp_signed_entry', 'sl_signed_entry', 'signed_heatmap_entry', 'heatmap_magnitude_entry', 'heatmap_direction_entry']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "time",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "high",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tick_volume",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "low_time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "high_time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ma_fast",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ma_slow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "signal",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "signal_shift",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "entry_long",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entry_short",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entry_signal",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "outcome_label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "in_trade",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "trade_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tp_heatmap",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sl_heatmap",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "combined_heatmap",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tp_prob",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sl_prob",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nohit_prob",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tp_prob_raw",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sl_prob_raw",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "signed_heatmap",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tp_signed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sl_signed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "heatmap_magnitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "heatmap_direction",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tp_heatmap_only",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sl_heatmap_only",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tp_signed_entry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sl_signed_entry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "signed_heatmap_entry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "heatmap_magnitude_entry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "heatmap_direction_entry",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b0c73be1-ae06-4d27-9ef1-107ba3fc6eb5",
       "rows": [
        [
         "2015-01-02 12:00:00",
         "1.20467",
         "1.206",
         "1.20347",
         "1.20377",
         "14827",
         "2015-01-02 15:36:00",
         "2015-01-02 12:48:00",
         null,
         null,
         "0",
         null,
         "0",
         "0",
         "0",
         "0",
         "0",
         "-1",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2015-01-02 16:00:00",
         "1.20377",
         "1.20437",
         "1.20069",
         "1.20131",
         "19160",
         "2015-01-02 19:36:00",
         "2015-01-02 16:00:00",
         null,
         null,
         "0",
         "0.0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "-1",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2015-01-02 20:00:00",
         "1.20133",
         "1.20154",
         "1.20011",
         "1.20029",
         "9335",
         "2015-01-02 23:12:00",
         "2015-01-02 20:00:00",
         null,
         null,
         "0",
         "0.0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "-1",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2015-01-03 00:00:00",
         "1.2003",
         "1.20055",
         "1.19997",
         "1.20014",
         "1351",
         "2015-01-03 00:24:00",
         "2015-01-03 00:00:00",
         null,
         null,
         "0",
         "0.0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "-1",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2015-01-05 00:00:00",
         "1.19454",
         "1.19755",
         "1.18642",
         "1.19624",
         "13941",
         "2015-01-05 00:48:00",
         "2015-01-05 03:36:00",
         null,
         null,
         "0",
         "0.0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "-1",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.0",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 37,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>tick_volume</th>\n",
       "      <th>low_time</th>\n",
       "      <th>high_time</th>\n",
       "      <th>ma_fast</th>\n",
       "      <th>ma_slow</th>\n",
       "      <th>signal</th>\n",
       "      <th>signal_shift</th>\n",
       "      <th>entry_long</th>\n",
       "      <th>entry_short</th>\n",
       "      <th>entry_signal</th>\n",
       "      <th>outcome_label</th>\n",
       "      <th>in_trade</th>\n",
       "      <th>trade_id</th>\n",
       "      <th>tp_heatmap</th>\n",
       "      <th>sl_heatmap</th>\n",
       "      <th>combined_heatmap</th>\n",
       "      <th>tp_prob</th>\n",
       "      <th>sl_prob</th>\n",
       "      <th>nohit_prob</th>\n",
       "      <th>tp_prob_raw</th>\n",
       "      <th>sl_prob_raw</th>\n",
       "      <th>signed_heatmap</th>\n",
       "      <th>tp_signed</th>\n",
       "      <th>sl_signed</th>\n",
       "      <th>heatmap_magnitude</th>\n",
       "      <th>heatmap_direction</th>\n",
       "      <th>tp_heatmap_only</th>\n",
       "      <th>sl_heatmap_only</th>\n",
       "      <th>tp_signed_entry</th>\n",
       "      <th>sl_signed_entry</th>\n",
       "      <th>signed_heatmap_entry</th>\n",
       "      <th>heatmap_magnitude_entry</th>\n",
       "      <th>heatmap_direction_entry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02 12:00:00</th>\n",
       "      <td>1.20467</td>\n",
       "      <td>1.20600</td>\n",
       "      <td>1.20347</td>\n",
       "      <td>1.20377</td>\n",
       "      <td>14827</td>\n",
       "      <td>2015-01-02 15:36:00</td>\n",
       "      <td>2015-01-02 12:48:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 16:00:00</th>\n",
       "      <td>1.20377</td>\n",
       "      <td>1.20437</td>\n",
       "      <td>1.20069</td>\n",
       "      <td>1.20131</td>\n",
       "      <td>19160</td>\n",
       "      <td>2015-01-02 19:36:00</td>\n",
       "      <td>2015-01-02 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 20:00:00</th>\n",
       "      <td>1.20133</td>\n",
       "      <td>1.20154</td>\n",
       "      <td>1.20011</td>\n",
       "      <td>1.20029</td>\n",
       "      <td>9335</td>\n",
       "      <td>2015-01-02 23:12:00</td>\n",
       "      <td>2015-01-02 20:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 00:00:00</th>\n",
       "      <td>1.20030</td>\n",
       "      <td>1.20055</td>\n",
       "      <td>1.19997</td>\n",
       "      <td>1.20014</td>\n",
       "      <td>1351</td>\n",
       "      <td>2015-01-03 00:24:00</td>\n",
       "      <td>2015-01-03 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05 00:00:00</th>\n",
       "      <td>1.19454</td>\n",
       "      <td>1.19755</td>\n",
       "      <td>1.18642</td>\n",
       "      <td>1.19624</td>\n",
       "      <td>13941</td>\n",
       "      <td>2015-01-05 00:48:00</td>\n",
       "      <td>2015-01-05 03:36:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high      low    close  tick_volume  \\\n",
       "time                                                                   \n",
       "2015-01-02 12:00:00  1.20467  1.20600  1.20347  1.20377        14827   \n",
       "2015-01-02 16:00:00  1.20377  1.20437  1.20069  1.20131        19160   \n",
       "2015-01-02 20:00:00  1.20133  1.20154  1.20011  1.20029         9335   \n",
       "2015-01-03 00:00:00  1.20030  1.20055  1.19997  1.20014         1351   \n",
       "2015-01-05 00:00:00  1.19454  1.19755  1.18642  1.19624        13941   \n",
       "\n",
       "                                low_time            high_time  ma_fast  \\\n",
       "time                                                                     \n",
       "2015-01-02 12:00:00  2015-01-02 15:36:00  2015-01-02 12:48:00      NaN   \n",
       "2015-01-02 16:00:00  2015-01-02 19:36:00  2015-01-02 16:00:00      NaN   \n",
       "2015-01-02 20:00:00  2015-01-02 23:12:00  2015-01-02 20:00:00      NaN   \n",
       "2015-01-03 00:00:00  2015-01-03 00:24:00  2015-01-03 00:00:00      NaN   \n",
       "2015-01-05 00:00:00  2015-01-05 00:48:00  2015-01-05 03:36:00      NaN   \n",
       "\n",
       "                     ma_slow  signal  signal_shift  entry_long  entry_short  \\\n",
       "time                                                                          \n",
       "2015-01-02 12:00:00      NaN       0           NaN           0            0   \n",
       "2015-01-02 16:00:00      NaN       0           0.0           0            0   \n",
       "2015-01-02 20:00:00      NaN       0           0.0           0            0   \n",
       "2015-01-03 00:00:00      NaN       0           0.0           0            0   \n",
       "2015-01-05 00:00:00      NaN       0           0.0           0            0   \n",
       "\n",
       "                     entry_signal  outcome_label  in_trade  trade_id  \\\n",
       "time                                                                   \n",
       "2015-01-02 12:00:00             0              0         0        -1   \n",
       "2015-01-02 16:00:00             0              0         0        -1   \n",
       "2015-01-02 20:00:00             0              0         0        -1   \n",
       "2015-01-03 00:00:00             0              0         0        -1   \n",
       "2015-01-05 00:00:00             0              0         0        -1   \n",
       "\n",
       "                     tp_heatmap  sl_heatmap  combined_heatmap  tp_prob  \\\n",
       "time                                                                     \n",
       "2015-01-02 12:00:00         0.0         0.0               0.0      0.0   \n",
       "2015-01-02 16:00:00         0.0         0.0               0.0      0.0   \n",
       "2015-01-02 20:00:00         0.0         0.0               0.0      0.0   \n",
       "2015-01-03 00:00:00         0.0         0.0               0.0      0.0   \n",
       "2015-01-05 00:00:00         0.0         0.0               0.0      0.0   \n",
       "\n",
       "                     sl_prob  nohit_prob  tp_prob_raw  sl_prob_raw  \\\n",
       "time                                                                 \n",
       "2015-01-02 12:00:00      0.0         1.0          0.0          0.0   \n",
       "2015-01-02 16:00:00      0.0         1.0          0.0          0.0   \n",
       "2015-01-02 20:00:00      0.0         1.0          0.0          0.0   \n",
       "2015-01-03 00:00:00      0.0         1.0          0.0          0.0   \n",
       "2015-01-05 00:00:00      0.0         1.0          0.0          0.0   \n",
       "\n",
       "                     signed_heatmap  tp_signed  sl_signed  heatmap_magnitude  \\\n",
       "time                                                                           \n",
       "2015-01-02 12:00:00             0.0        0.0       -0.0                0.0   \n",
       "2015-01-02 16:00:00             0.0        0.0       -0.0                0.0   \n",
       "2015-01-02 20:00:00             0.0        0.0       -0.0                0.0   \n",
       "2015-01-03 00:00:00             0.0        0.0       -0.0                0.0   \n",
       "2015-01-05 00:00:00             0.0        0.0       -0.0                0.0   \n",
       "\n",
       "                     heatmap_direction  tp_heatmap_only  sl_heatmap_only  \\\n",
       "time                                                                       \n",
       "2015-01-02 12:00:00                0.0              0.0              0.0   \n",
       "2015-01-02 16:00:00                0.0              0.0              0.0   \n",
       "2015-01-02 20:00:00                0.0              0.0              0.0   \n",
       "2015-01-03 00:00:00                0.0              0.0              0.0   \n",
       "2015-01-05 00:00:00                0.0              0.0              0.0   \n",
       "\n",
       "                     tp_signed_entry  sl_signed_entry  signed_heatmap_entry  \\\n",
       "time                                                                          \n",
       "2015-01-02 12:00:00              0.0             -0.0                   0.0   \n",
       "2015-01-02 16:00:00              0.0             -0.0                   0.0   \n",
       "2015-01-02 20:00:00              0.0             -0.0                   0.0   \n",
       "2015-01-03 00:00:00              0.0             -0.0                   0.0   \n",
       "2015-01-05 00:00:00              0.0             -0.0                   0.0   \n",
       "\n",
       "                     heatmap_magnitude_entry  heatmap_direction_entry  \n",
       "time                                                                   \n",
       "2015-01-02 12:00:00                      0.0                      0.0  \n",
       "2015-01-02 16:00:00                      0.0                      0.0  \n",
       "2015-01-02 20:00:00                      0.0                      0.0  \n",
       "2015-01-03 00:00:00                      0.0                      0.0  \n",
       "2015-01-05 00:00:00                      0.0                      0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the prepared dataset with entry-based heatmaps\n",
    "df = pd.read_csv('../DATA/eurusd_with_heatmaps.csv', parse_dates=True, index_col='time')\n",
    "events_df = pd.read_csv('../DATA/eurusd_barrier_events.csv', parse_dates=['entry_time', 'exit_time'])\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "print(f\"\\nTotal events: {len(events_df)}\")\n",
    "print(f\"\\nAvailable columns: {df.columns.tolist()}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d234fb9",
   "metadata": {},
   "source": [
    "## 2. Import Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba41ad69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FeaturesLibrary imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import existing feature engineering functions\n",
    "try:\n",
    "    from FeaturesLibrary import *\n",
    "    print(\"✅ FeaturesLibrary imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Could not import FeaturesLibrary: {e}\")\n",
    "    print(\"We'll define basic features manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df570d9",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We'll create features from multiple categories:\n",
    "- **Price features**: Returns, log returns, OHLC relationships\n",
    "- **Volatility**: Parkinson, Yang-Zhang estimators\n",
    "- **Technical**: RSI, ATR, Bollinger Bands\n",
    "- **Statistical**: Rolling mean, std, skewness\n",
    "- **Derivatives**: Velocity, acceleration of price\n",
    "- **Market regime**: KAMA-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec9c441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After feature engineering: (13676, 65)\n",
      "\n",
      "New features created: 55\n",
      "\n",
      "Feature columns sample: ['tick_volume', 'low_time', 'high_time', 'ma_fast', 'ma_slow', 'signal', 'signal_shift', 'entry_long', 'entry_short', 'entry_signal', 'outcome_label', 'in_trade', 'trade_id', 'tp_heatmap', 'sl_heatmap']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "time",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "returns",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hl_spread",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "returns_std_20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rsi_20",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ae5ee1cb-9321-46bd-950a-06ef0490714c",
       "rows": [
        [
         "2023-09-19 16:00:00",
         "1.06848",
         "-0.0013832292795992185",
         "0.0021432315064391708",
         "0.0017915084897126305",
         "40.80029612563272"
        ],
        [
         "2023-09-19 20:00:00",
         "1.0679",
         "-0.0005428271937706342",
         "0.0014701751100290956",
         "0.0013360789347329704",
         "50.50925458247667"
        ],
        [
         "2023-09-20 00:00:00",
         "1.06849",
         "0.0005524861878452025",
         "0.0008610281799549195",
         "0.0011834859529483597",
         "59.400820309832135"
        ],
        [
         "2023-09-20 04:00:00",
         "1.06786",
         "-0.000589617123229913",
         "0.0010675556720917713",
         "0.001188476056695148",
         "58.85362754824692"
        ],
        [
         "2023-09-20 08:00:00",
         "1.06964",
         "0.0016668851722134725",
         "0.00226244343891411",
         "0.001231770370314281",
         "62.78959217119676"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>returns</th>\n",
       "      <th>hl_spread</th>\n",
       "      <th>returns_std_20</th>\n",
       "      <th>rsi_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-09-19 16:00:00</th>\n",
       "      <td>1.06848</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>40.800296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-19 20:00:00</th>\n",
       "      <td>1.06790</td>\n",
       "      <td>-0.000543</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>50.509255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20 00:00:00</th>\n",
       "      <td>1.06849</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>59.400820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20 04:00:00</th>\n",
       "      <td>1.06786</td>\n",
       "      <td>-0.000590</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>58.853628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20 08:00:00</th>\n",
       "      <td>1.06964</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>62.789592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       close   returns  hl_spread  returns_std_20     rsi_20\n",
       "time                                                                        \n",
       "2023-09-19 16:00:00  1.06848 -0.001383   0.002143        0.001792  40.800296\n",
       "2023-09-19 20:00:00  1.06790 -0.000543   0.001470        0.001336  50.509255\n",
       "2023-09-20 00:00:00  1.06849  0.000552   0.000861        0.001183  59.400820\n",
       "2023-09-20 04:00:00  1.06786 -0.000590   0.001068        0.001188  58.853628\n",
       "2023-09-20 08:00:00  1.06964  0.001667   0.002262        0.001232  62.789592"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_basic_features(df, windows=[5, 10, 20, 50]):\n",
    "    \"\"\"\n",
    "    Add basic technical features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # OHLC features\n",
    "    df['hl_spread'] = (df['high'] - df['low']) / df['close']\n",
    "    df['co_spread'] = (df['close'] - df['open']) / df['open']\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for window in windows:\n",
    "        df[f'returns_mean_{window}'] = df['returns'].rolling(window).mean()\n",
    "        df[f'returns_std_{window}'] = df['returns'].rolling(window).std()\n",
    "        df[f'close_std_{window}'] = df['close'].rolling(window).std()\n",
    "        \n",
    "        # Volume features (using tick_volume)\n",
    "        if 'tick_volume' in df.columns:\n",
    "            df[f'tick_volume_mean_{window}'] = df['tick_volume'].rolling(window).mean()\n",
    "            df[f'tick_volume_std_{window}'] = df['tick_volume'].rolling(window).std()\n",
    "        \n",
    "        # RSI-like momentum\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        df[f'rsi_{window}'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply features\n",
    "df = add_basic_features(df, windows=[5, 10, 20, 50])\n",
    "\n",
    "print(f\"After feature engineering: {df.shape}\")\n",
    "print(f\"\\nNew features created: {df.shape[1] - len(df.columns[:10])}\")\n",
    "\n",
    "# Show some features\n",
    "feature_cols = [col for col in df.columns if col not in ['open', 'high', 'low', 'close', 'volume']]\n",
    "print(f\"\\nFeature columns sample: {feature_cols[:15]}\")\n",
    "\n",
    "df[['close', 'returns', 'hl_spread', 'returns_std_20', 'rsi_20']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d777448",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa080a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values by column:\n",
      "                     count  percentage\n",
      "returns_std_50          50    0.365604\n",
      "returns_mean_50         50    0.365604\n",
      "rsi_50                  49    0.358292\n",
      "ma_slow                 49    0.358292\n",
      "tick_volume_std_50      49    0.358292\n",
      "tick_volume_mean_50     49    0.358292\n",
      "close_std_50            49    0.358292\n",
      "returns_std_20          20    0.146242\n",
      "returns_mean_20         20    0.146242\n",
      "rsi_20                  19    0.138930\n",
      "\n",
      "Before dropping NaN: 13676 rows\n",
      "After dropping NaN: 13626 rows\n",
      "Rows dropped: 50\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "print(\"Missing values by column:\")\n",
    "missing_df = pd.DataFrame({'count': missing_counts, 'percentage': missing_pct})\n",
    "missing_df = missing_df[missing_df['count'] > 0].sort_values('count', ascending=False)\n",
    "print(missing_df.head(10))\n",
    "\n",
    "# Drop initial rows with NaN (from rolling windows)\n",
    "print(f\"\\nBefore dropping NaN: {len(df)} rows\")\n",
    "df_clean = df.dropna().copy()\n",
    "print(f\"After dropping NaN: {len(df_clean)} rows\")\n",
    "print(f\"Rows dropped: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d72ecc",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "\n",
    "Select relevant features for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature columns: 30\n",
      "\n",
      "Feature columns:\n",
      "  1. low_time\n",
      "  2. high_time\n",
      "  3. returns\n",
      "  4. log_returns\n",
      "  5. hl_spread\n",
      "  6. co_spread\n",
      "  7. returns_mean_5\n",
      "  8. returns_std_5\n",
      "  9. close_std_5\n",
      "  10. tick_volume_mean_5\n",
      "  11. tick_volume_std_5\n",
      "  12. rsi_5\n",
      "  13. returns_mean_10\n",
      "  14. returns_std_10\n",
      "  15. close_std_10\n",
      "  16. tick_volume_mean_10\n",
      "  17. tick_volume_std_10\n",
      "  18. rsi_10\n",
      "  19. returns_mean_20\n",
      "  20. returns_std_20\n",
      "  21. close_std_20\n",
      "  22. tick_volume_mean_20\n",
      "  23. tick_volume_std_20\n",
      "  24. rsi_20\n",
      "  25. returns_mean_50\n",
      "  26. returns_std_50\n",
      "  27. close_std_50\n",
      "  28. tick_volume_mean_50\n",
      "  29. tick_volume_std_50\n",
      "  30. rsi_50\n",
      "\n",
      "Feature matrix X shape: (13626, 30)\n",
      "Target vector y shape: (13626,)\n",
      "\n",
      "Target distribution:\n",
      "  Min: -1.0000\n",
      "  Max: 1.0000\n",
      "  Mean: -0.0317\n",
      "  Std: 0.2858\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns (exclude raw OHLCV, labels, and intermediate columns)\n",
    "exclude_cols = [\n",
    "    'open', 'high', 'low', 'close', 'volume', 'tick_volume',  # Raw OHLCV\n",
    "    'low_time', 'high_time',  # Datetime columns\n",
    "    'ma_fast', 'ma_slow', 'signal', 'signal_shift', 'entry_long', 'entry_short', 'entry_signal',  # Signal generation\n",
    "    'outcome_label', 'in_trade', 'trade_id',  # Labels\n",
    "    'tp_heatmap', 'sl_heatmap', 'combined_heatmap',  # Old exit-based heatmaps\n",
    "    'tp_prob', 'sl_prob', 'nohit_prob', 'tp_prob_raw', 'sl_prob_raw',  # 3-class approach\n",
    "    'signed_heatmap', 'tp_signed', 'sl_signed', 'heatmap_magnitude', 'heatmap_direction',  # Old exit-based\n",
    "    'tp_heatmap_only', 'sl_heatmap_only',  # Intermediate\n",
    "    'tp_signed_entry', 'sl_signed_entry', 'signed_heatmap_entry',  # Target columns\n",
    "    'heatmap_magnitude_entry', 'heatmap_direction_entry'  # Alternative targets\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in df_clean.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total feature columns: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = df_clean[feature_cols].copy()\n",
    "y = df_clean['signed_heatmap_entry'].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix X shape: {X.shape}\")\n",
    "print(f\"Target vector y shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Min: {y.min():.4f}\")\n",
    "print(f\"  Max: {y.max():.4f}\")\n",
    "print(f\"  Mean: {y.mean():.4f}\")\n",
    "print(f\"  Std: {y.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64def5",
   "metadata": {},
   "source": [
    "## 6. Create Sliding Windows for Sequence Modeling\n",
    "\n",
    "For LSTM/CNN models, we need sequences of past observations to predict the current outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8afbb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence shape: (13566, 60, 30)\n",
      "  - Number of sequences: 13,566\n",
      "  - Window length: 60 timesteps\n",
      "  - Number of features: 30\n",
      "\n",
      "Target shape: (13566,)\n",
      "\n",
      "Target distribution in sequences:\n",
      "  Min: -1.0000\n",
      "  Max: 1.0000\n",
      "  Mean: -0.0320\n",
      "  Std: 0.2863\n",
      "\n",
      "Non-zero targets: 2,734 (20.2%)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(X, y, window_length=60, stride=1):\n",
    "    \"\"\"\n",
    "    Create sliding window sequences for time series modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame of features (n_samples, n_features)\n",
    "    y : Series of targets (n_samples,)\n",
    "    window_length : Number of past timesteps to include\n",
    "    stride : Step size between consecutive windows\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_seq : array (n_sequences, window_length, n_features)\n",
    "    y_seq : array (n_sequences,)\n",
    "    indices : array of end indices for each sequence\n",
    "    \"\"\"\n",
    "    X_array = X.values\n",
    "    y_array = y.values\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(window_length, len(X), stride):\n",
    "        X_sequences.append(X_array[i-window_length:i])\n",
    "        y_sequences.append(y_array[i])  # Predict current timestep\n",
    "        indices.append(i)\n",
    "    \n",
    "    X_seq = np.array(X_sequences)\n",
    "    y_seq = np.array(y_sequences)\n",
    "    indices = np.array(indices)\n",
    "    \n",
    "    return X_seq, y_seq, indices\n",
    "\n",
    "# Create sequences\n",
    "window_length = 60  # 60 bars = 10 days at 4H timeframe\n",
    "stride = 1  # No overlap reduction for maximum data\n",
    "\n",
    "X_seq, y_seq, seq_indices = create_sequences(X, y, window_length=window_length, stride=stride)\n",
    "\n",
    "print(f\"Sequence shape: {X_seq.shape}\")\n",
    "print(f\"  - Number of sequences: {X_seq.shape[0]:,}\")\n",
    "print(f\"  - Window length: {X_seq.shape[1]} timesteps\")\n",
    "print(f\"  - Number of features: {X_seq.shape[2]}\")\n",
    "print(f\"\\nTarget shape: {y_seq.shape}\")\n",
    "print(f\"\\nTarget distribution in sequences:\")\n",
    "print(f\"  Min: {y_seq.min():.4f}\")\n",
    "print(f\"  Max: {y_seq.max():.4f}\")\n",
    "print(f\"  Mean: {y_seq.mean():.4f}\")\n",
    "print(f\"  Std: {y_seq.std():.4f}\")\n",
    "print(f\"\\nNon-zero targets: {(np.abs(y_seq) > 0.1).sum():,} ({(np.abs(y_seq) > 0.1).sum()/len(y_seq):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e942982",
   "metadata": {},
   "source": [
    "## 7. Train/Validation/Test Split\n",
    "\n",
    "For time series, we use temporal split (no shuffling!):\n",
    "- Train: First 70%\n",
    "- Validation: Next 15%\n",
    "- Test: Last 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f03ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "  Train: 9,496 sequences (70.0%)\n",
      "  Val:   2,034 sequences (15.0%)\n",
      "  Test:  2,036 sequences (15.0%)\n",
      "\n",
      "Temporal ranges:\n",
      "  Train: 2015-01-28 04:00:00 to 2021-02-11 00:00:00\n",
      "  Val:   2021-02-11 04:00:00 to 2022-06-01 00:00:00\n",
      "  Test:  2022-06-01 04:00:00 to 2023-09-20 08:00:00\n",
      "\n",
      "Target statistics by split:\n",
      "  Train - Mean: -0.0226, Std: 0.2842, Non-zero: 19.9%\n",
      "  Val   - Mean: -0.0582, Std: 0.2822, Non-zero: 19.9%\n",
      "  Test  - Mean: -0.0498, Std: 0.2973, Non-zero: 21.9%\n"
     ]
    }
   ],
   "source": [
    "# Temporal split\n",
    "n_total = len(X_seq)\n",
    "n_train = int(n_total * 0.70)\n",
    "n_val = int(n_total * 0.15)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "X_train = X_seq[:n_train]\n",
    "y_train = y_seq[:n_train]\n",
    "\n",
    "X_val = X_seq[n_train:n_train+n_val]\n",
    "y_val = y_seq[n_train:n_train+n_val]\n",
    "\n",
    "X_test = X_seq[n_train+n_val:]\n",
    "y_test = y_seq[n_train+n_val:]\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Train: {X_train.shape[0]:,} sequences ({n_train/n_total:.1%})\")\n",
    "print(f\"  Val:   {X_val.shape[0]:,} sequences ({n_val/n_total:.1%})\")\n",
    "print(f\"  Test:  {X_test.shape[0]:,} sequences ({n_test/n_total:.1%})\")\n",
    "\n",
    "# Get corresponding dates\n",
    "train_indices = seq_indices[:n_train]\n",
    "val_indices = seq_indices[n_train:n_train+n_val]\n",
    "test_indices = seq_indices[n_train+n_val:]\n",
    "\n",
    "train_dates = df_clean.index[train_indices]\n",
    "val_dates = df_clean.index[val_indices]\n",
    "test_dates = df_clean.index[test_indices]\n",
    "\n",
    "print(f\"\\nTemporal ranges:\")\n",
    "print(f\"  Train: {train_dates[0]} to {train_dates[-1]}\")\n",
    "print(f\"  Val:   {val_dates[0]} to {val_dates[-1]}\")\n",
    "print(f\"  Test:  {test_dates[0]} to {test_dates[-1]}\")\n",
    "\n",
    "# Check target distribution in each split\n",
    "print(f\"\\nTarget statistics by split:\")\n",
    "print(f\"  Train - Mean: {y_train.mean():.4f}, Std: {y_train.std():.4f}, Non-zero: {(np.abs(y_train)>0.1).sum()/len(y_train):.1%}\")\n",
    "print(f\"  Val   - Mean: {y_val.mean():.4f}, Std: {y_val.std():.4f}, Non-zero: {(np.abs(y_val)>0.1).sum()/len(y_val):.1%}\")\n",
    "print(f\"  Test  - Mean: {y_test.mean():.4f}, Std: {y_test.std():.4f}, Non-zero: {(np.abs(y_test)>0.1).sum()/len(y_test):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3be41f",
   "metadata": {},
   "source": [
    "## 8. Feature Normalization\n",
    "\n",
    "Normalize features using statistics from training set only (avoid lookahead bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "263de555",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2015-01-14 12:24:00'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Fit on training data\u001b[39;00m\n\u001b[32m     14\u001b[39m scaler = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m X_train_scaled_2d = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m X_val_scaled_2d = scaler.transform(X_val_2d)\n\u001b[32m     17\u001b[39m X_test_scaled_2d = scaler.transform(X_test_2d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jloya\\Documents\\1-week-1-bot-challenge\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jloya\\Documents\\1-week-1-bot-challenge\\.venv\\Lib\\site-packages\\sklearn\\base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jloya\\Documents\\1-week-1-bot-challenge\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:907\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jloya\\Documents\\1-week-1-bot-challenge\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jloya\\Documents\\1-week-1-bot-challenge\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:943\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    912\u001b[39m \n\u001b[32m    913\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    942\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jloya\\Documents\\1-week-1-bot-challenge\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jloya\\Documents\\1-week-1-bot-challenge\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1053\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1051\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1056\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jloya\\Documents\\1-week-1-bot-challenge\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:757\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    755\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '2015-01-14 12:24:00'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fit scaler on training data only\n",
    "# Reshape to 2D for scaler\n",
    "n_train_samples = X_train.shape[0]\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "X_train_2d = X_train.reshape(-1, n_features)\n",
    "X_val_2d = X_val.reshape(-1, n_features)\n",
    "X_test_2d = X_test.reshape(-1, n_features)\n",
    "\n",
    "# Fit on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_2d = scaler.fit_transform(X_train_2d)\n",
    "X_val_scaled_2d = scaler.transform(X_val_2d)\n",
    "X_test_scaled_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train_scaled = X_train_scaled_2d.reshape(X_train.shape[0], n_timesteps, n_features)\n",
    "X_val_scaled = X_val_scaled_2d.reshape(X_val.shape[0], n_timesteps, n_features)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(X_test.shape[0], n_timesteps, n_features)\n",
    "\n",
    "print(\"✅ Feature normalization complete\")\n",
    "print(f\"\\nScaled train set stats:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.6f} (should be ~1)\")\n",
    "print(f\"\\nScaled val set stats:\")\n",
    "print(f\"  Mean: {X_val_scaled.mean():.6f}\")\n",
    "print(f\"  Std:  {X_val_scaled.std():.6f}\")\n",
    "print(f\"\\nScaled test set stats:\")\n",
    "print(f\"  Mean: {X_test_scaled.mean():.6f}\")\n",
    "print(f\"  Std:  {X_test_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea2ccca",
   "metadata": {},
   "source": [
    "## 9. Visualize Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Histogram of targets\n",
    "axes[0, 0].hist(y_train, bins=50, alpha=0.7, label='Train', color='blue')\n",
    "axes[0, 0].hist(y_val, bins=50, alpha=0.7, label='Val', color='orange')\n",
    "axes[0, 0].hist(y_test, bins=50, alpha=0.7, label='Test', color='green')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "axes[0, 0].set_xlabel('Target Value (signed_heatmap_entry)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Targets Across Splits')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Time series of targets (training set)\n",
    "axes[0, 1].plot(train_dates, y_train, linewidth=0.5, alpha=0.7)\n",
    "axes[0, 1].fill_between(train_dates, 0, y_train, where=(y_train > 0), color='green', alpha=0.3, label='Good Bets')\n",
    "axes[0, 1].fill_between(train_dates, 0, y_train, where=(y_train < 0), color='red', alpha=0.3, label='Bad Bets')\n",
    "axes[0, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Target Value')\n",
    "axes[0, 1].set_title('Training Set: Targets Over Time')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Cumulative distribution\n",
    "for y_split, label, color in [(y_train, 'Train', 'blue'), (y_val, 'Val', 'orange'), (y_test, 'Test', 'green')]:\n",
    "    sorted_y = np.sort(y_split)\n",
    "    cumulative = np.arange(1, len(sorted_y) + 1) / len(sorted_y)\n",
    "    axes[1, 0].plot(sorted_y, cumulative, label=label, color=color, linewidth=2)\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Target Value')\n",
    "axes[1, 0].set_ylabel('Cumulative Probability')\n",
    "axes[1, 0].set_title('Cumulative Distribution of Targets')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Class balance (discretized)\n",
    "def discretize_targets(y, threshold=0.1):\n",
    "    discrete = np.zeros_like(y)\n",
    "    discrete[y > threshold] = 1  # Good bets\n",
    "    discrete[y < -threshold] = -1  # Bad bets\n",
    "    return discrete\n",
    "\n",
    "y_train_discrete = discretize_targets(y_train)\n",
    "y_val_discrete = discretize_targets(y_val)\n",
    "y_test_discrete = discretize_targets(y_test)\n",
    "\n",
    "labels = ['Bad Bet (-1)', 'Noise (0)', 'Good Bet (+1)']\n",
    "train_counts = [(y_train_discrete == i).sum() for i in [-1, 0, 1]]\n",
    "val_counts = [(y_val_discrete == i).sum() for i in [-1, 0, 1]]\n",
    "test_counts = [(y_test_discrete == i).sum() for i in [-1, 0, 1]]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 1].bar(x - width, train_counts, width, label='Train', color='blue', alpha=0.7)\n",
    "axes[1, 1].bar(x, val_counts, width, label='Val', color='orange', alpha=0.7)\n",
    "axes[1, 1].bar(x + width, test_counts, width, label='Test', color='green', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Class')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Class Balance (threshold=0.1)')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(labels)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASS BALANCE (threshold=0.1):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Train: Bad={train_counts[0]:,} ({train_counts[0]/len(y_train):.1%}), \"\n",
    "      f\"Noise={train_counts[1]:,} ({train_counts[1]/len(y_train):.1%}), \"\n",
    "      f\"Good={train_counts[2]:,} ({train_counts[2]/len(y_train):.1%})\")\n",
    "print(f\"Val:   Bad={val_counts[0]:,} ({val_counts[0]/len(y_val):.1%}), \"\n",
    "      f\"Noise={val_counts[1]:,} ({val_counts[1]/len(y_val):.1%}), \"\n",
    "      f\"Good={val_counts[2]:,} ({val_counts[2]/len(y_val):.1%})\")\n",
    "print(f\"Test:  Bad={test_counts[0]:,} ({test_counts[0]/len(y_test):.1%}), \"\n",
    "      f\"Noise={test_counts[1]:,} ({test_counts[1]/len(y_test):.1%}), \"\n",
    "      f\"Good={test_counts[2]:,} ({test_counts[2]/len(y_test):.1%})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043abea4",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed arrays\n",
    "np.save('../DATA/X_train_scaled.npy', X_train_scaled)\n",
    "np.save('../DATA/X_val_scaled.npy', X_val_scaled)\n",
    "np.save('../DATA/X_test_scaled.npy', X_test_scaled)\n",
    "\n",
    "np.save('../DATA/y_train.npy', y_train)\n",
    "np.save('../DATA/y_val.npy', y_val)\n",
    "np.save('../DATA/y_test.npy', y_test)\n",
    "\n",
    "# Save scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, '../DATA/feature_scaler.pkl')\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'window_length': window_length,\n",
    "    'stride': stride,\n",
    "    'n_features': n_features,\n",
    "    'feature_names': feature_cols,\n",
    "    'n_train': len(X_train),\n",
    "    'n_val': len(X_val),\n",
    "    'n_test': len(X_test),\n",
    "    'train_date_range': (str(train_dates[0]), str(train_dates[-1])),\n",
    "    'val_date_range': (str(val_dates[0]), str(val_dates[-1])),\n",
    "    'test_date_range': (str(test_dates[0]), str(test_dates[-1]))\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../DATA/data_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"✅ All data saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - X_train_scaled.npy, X_val_scaled.npy, X_test_scaled.npy\")\n",
    "print(\"  - y_train.npy, y_val.npy, y_test.npy\")\n",
    "print(\"  - feature_scaler.pkl\")\n",
    "print(\"  - data_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0268b0f",
   "metadata": {},
   "source": [
    "## 🎯 Phase 2 Complete!\n",
    "\n",
    "### What we accomplished:\n",
    "1. ✅ Loaded data from Phase 1 with entry-based heatmaps\n",
    "2. ✅ Engineered technical features (returns, volatility, RSI, etc.)\n",
    "3. ✅ Created sliding window sequences (60 timesteps)\n",
    "4. ✅ Split data temporally (70/15/15 train/val/test)\n",
    "5. ✅ Normalized features using training set statistics\n",
    "6. ✅ Saved processed data for model training\n",
    "\n",
    "### Data ready for Phase 3:\n",
    "- **X_train**: Shape `(n_train, 60, n_features)` - normalized feature sequences\n",
    "- **y_train**: Shape `(n_train,)` - entry-based heatmap labels [-1, +1]\n",
    "- **Class balance**: ~72% noise, ~18% bad bets, ~10% good bets\n",
    "\n",
    "### Next Steps (Phase 3):\n",
    "- Build dual-branch LSTM/CNN model\n",
    "- Branch 1: Entry signal confidence\n",
    "- Branch 2: Outcome prediction (heatmap)\n",
    "- Combined architecture for meta-labeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
