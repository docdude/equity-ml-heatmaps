{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eae3380",
   "metadata": {},
   "source": [
    "# Phase 6: Multi-Asset Scaling\n",
    "\n",
    "## Problem & Solution\n",
    "**Phase 5 achieved excellent results** but with limited data:\n",
    "- Train: 9,496 samples (EURUSD only)\n",
    "- Strong signal direction: **67.3%** accuracy\n",
    "- Overfitting: **CRUSHED** (gap = -0.018)\n",
    "\n",
    "**Scaling Strategy:**\n",
    "1. Load **5 FX pairs** (EURUSD, GBPUSD, USDJPY, USDCAD, USDCHF)\n",
    "2. Combine all data → **~50,000 samples**\n",
    "3. Add asset embedding (one-hot encoding per pair)\n",
    "4. Train larger model without overfitting\n",
    "5. Test generalization per asset\n",
    "\n",
    "## Expected Benefits:\n",
    "- **10x more strong signals** for training\n",
    "- **Cross-asset patterns** (USD strength, volatility regimes)\n",
    "- **Better calibration** of heatmap predictions\n",
    "- **Increased model capacity** without overfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaddf2d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4121ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, callbacks, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d8d99",
   "metadata": {},
   "source": [
    "## 2. Load Multi-Asset Data\n",
    "\n",
    "Use the pre-processed `featured_labeled` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path('../DATA/4-hours Pepperstone')\n",
    "PHASE6_MODEL_DIR = Path('../MODELS_PHASE6')\n",
    "PHASE6_MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Asset pairs to load\n",
    "ASSETS = ['EURUSD', 'GBPUSD', 'USDJPY', 'USDCAD', 'USDCHF']\n",
    "\n",
    "# Load all featured datasets\n",
    "print(\"Loading multi-asset data...\")\n",
    "asset_data = {}\n",
    "\n",
    "for asset in ASSETS:\n",
    "    file_path = DATA_DIR / f'4-hours_{asset}_featured_labeled.csv'\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df['asset'] = asset  # Add asset identifier\n",
    "        asset_data[asset] = df\n",
    "        print(f\"  {asset}: {len(df)} samples\")\n",
    "    else:\n",
    "        print(f\"  {asset}: FILE NOT FOUND - {file_path}\")\n",
    "\n",
    "# Combine all assets\n",
    "if asset_data:\n",
    "    df_combined = pd.concat(asset_data.values(), axis=0, ignore_index=True)\n",
    "    print(f\"\\nTotal combined samples: {len(df_combined)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No featured_labeled datasets found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999b3e0",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Extract features and create sliding windows with asset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns (based on Phase 1-5)\n",
    "FEATURE_COLS = [\n",
    "    'log_return_1', 'log_return_5', 'log_return_20',\n",
    "    'rsi_14', 'rsi_50',\n",
    "    'rolling_mean_20', 'rolling_std_20', 'rolling_min_20', 'rolling_max_20',\n",
    "    'rolling_mean_50', 'rolling_std_50',\n",
    "    'ema_span_20', 'ema_span_50',\n",
    "    'macd', 'macd_signal', 'macd_diff',\n",
    "    'bb_upper', 'bb_middle', 'bb_lower', 'bb_width',\n",
    "    'atr_14',\n",
    "    'hour', 'day_of_week',\n",
    "    'volume_close'\n",
    "]\n",
    "\n",
    "TARGET_COL = 'signed_heatmap_entry'\n",
    "\n",
    "# Check which features exist\n",
    "available_features = [col for col in FEATURE_COLS if col in df_combined.columns]\n",
    "print(f\"\\nAvailable features: {len(available_features)}/{len(FEATURE_COLS)}\")\n",
    "print(f\"Features: {available_features}\")\n",
    "\n",
    "# Check target\n",
    "if TARGET_COL not in df_combined.columns:\n",
    "    print(f\"\\nERROR: Target column '{TARGET_COL}' not found!\")\n",
    "    print(f\"Available columns: {df_combined.columns.tolist()}\")\n",
    "else:\n",
    "    print(f\"\\nTarget column '{TARGET_COL}' found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create asset embeddings (one-hot encoding)\n",
    "asset_dummies = pd.get_dummies(df_combined['asset'], prefix='asset')\n",
    "df_combined = pd.concat([df_combined, asset_dummies], axis=1)\n",
    "\n",
    "# Update feature list to include asset embeddings\n",
    "asset_embedding_cols = asset_dummies.columns.tolist()\n",
    "print(f\"\\nAsset embeddings: {asset_embedding_cols}\")\n",
    "\n",
    "# Final feature list\n",
    "ALL_FEATURES = available_features + asset_embedding_cols\n",
    "print(f\"\\nTotal features (including asset embeddings): {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080dc9ab",
   "metadata": {},
   "source": [
    "## 4. Create Sliding Windows Per Asset\n",
    "\n",
    "Process each asset separately to avoid data leakage across assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1998ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(df, feature_cols, target_col, window_size=60, asset_name=None):\n",
    "    \"\"\"\n",
    "    Create sliding windows for time series data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with features and target\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name\n",
    "        window_size: Number of timesteps to look back\n",
    "        asset_name: Optional asset name for logging\n",
    "    \n",
    "    Returns:\n",
    "        X: Array of shape (n_samples, window_size, n_features)\n",
    "        y: Array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Drop NaN and inf\n",
    "    df_clean = df[feature_cols + [target_col]].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    if len(df_clean) < window_size + 1:\n",
    "        if asset_name:\n",
    "            print(f\"  {asset_name}: Not enough data ({len(df_clean)} samples)\")\n",
    "        return None, None\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for i in range(len(df_clean) - window_size):\n",
    "        # Window of features\n",
    "        X_window = df_clean[feature_cols].iloc[i:i+window_size].values\n",
    "        \n",
    "        # Target at next timestep\n",
    "        y_target = df_clean[target_col].iloc[i+window_size]\n",
    "        \n",
    "        X_list.append(X_window)\n",
    "        y_list.append(y_target)\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    if asset_name:\n",
    "        print(f\"  {asset_name}: {len(X)} windows created\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create windows for each asset\n",
    "WINDOW_SIZE = 60\n",
    "print(f\"Creating sliding windows (window_size={WINDOW_SIZE})...\\n\")\n",
    "\n",
    "X_by_asset = {}\n",
    "y_by_asset = {}\n",
    "\n",
    "for asset in ASSETS:\n",
    "    if asset in asset_data:\n",
    "        # Get asset-specific data with embeddings\n",
    "        asset_df = df_combined[df_combined['asset'] == asset].copy()\n",
    "        \n",
    "        # Create windows\n",
    "        X, y = create_sliding_windows(\n",
    "            asset_df, \n",
    "            ALL_FEATURES, \n",
    "            TARGET_COL, \n",
    "            window_size=WINDOW_SIZE,\n",
    "            asset_name=asset\n",
    "        )\n",
    "        \n",
    "        if X is not None:\n",
    "            X_by_asset[asset] = X\n",
    "            y_by_asset[asset] = y\n",
    "\n",
    "print(f\"\\nTotal assets processed: {len(X_by_asset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4de889",
   "metadata": {},
   "source": [
    "## 5. Train/Val/Test Split\n",
    "\n",
    "**Strategy**: \n",
    "- Train on first 70% of each asset\n",
    "- Val on next 15%\n",
    "- Test on last 15%\n",
    "- Combine all assets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f047d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"Time-series split\"\"\"\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    X_train = X[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    X_val = X[train_end:val_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    X_test = X[val_end:]\n",
    "    y_test = y[val_end:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Split each asset\n",
    "splits_by_asset = {}\n",
    "print(\"Splitting data by asset...\\n\")\n",
    "\n",
    "for asset in X_by_asset.keys():\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "        X_by_asset[asset], \n",
    "        y_by_asset[asset]\n",
    "    )\n",
    "    \n",
    "    splits_by_asset[asset] = {\n",
    "        'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
    "        'y_train': y_train, 'y_val': y_val, 'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    print(f\"{asset}:\")\n",
    "    print(f\"  Train: {len(X_train)} samples\")\n",
    "    print(f\"  Val: {len(X_val)} samples\")\n",
    "    print(f\"  Test: {len(X_test)} samples\")\n",
    "\n",
    "# Combine all training data\n",
    "X_train_combined = np.concatenate([s['X_train'] for s in splits_by_asset.values()], axis=0)\n",
    "y_train_combined = np.concatenate([s['y_train'] for s in splits_by_asset.values()], axis=0)\n",
    "X_val_combined = np.concatenate([s['X_val'] for s in splits_by_asset.values()], axis=0)\n",
    "y_val_combined = np.concatenate([s['y_val'] for s in splits_by_asset.values()], axis=0)\n",
    "X_test_combined = np.concatenate([s['X_test'] for s in splits_by_asset.values()], axis=0)\n",
    "y_test_combined = np.concatenate([s['y_test'] for s in splits_by_asset.values()], axis=0)\n",
    "\n",
    "print(f\"\\nCombined dataset:\")\n",
    "print(f\"  Train: {len(X_train_combined)} samples\")\n",
    "print(f\"  Val: {len(X_val_combined)} samples\")\n",
    "print(f\"  Test: {len(X_test_combined)} samples\")\n",
    "print(f\"  Features: {X_train_combined.shape[2]}\")\n",
    "print(f\"  Window size: {X_train_combined.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0bb55",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for scaling\n",
    "n_train, n_timesteps, n_features = X_train_combined.shape\n",
    "X_train_reshaped = X_train_combined.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_reshaped)\n",
    "\n",
    "# Scale all datasets\n",
    "def scale_data(X):\n",
    "    n_samples, n_steps, n_feat = X.shape\n",
    "    X_reshaped = X.reshape(-1, n_feat)\n",
    "    X_scaled = scaler.transform(X_reshaped)\n",
    "    return X_scaled.reshape(n_samples, n_steps, n_feat)\n",
    "\n",
    "X_train_scaled = scale_data(X_train_combined)\n",
    "X_val_scaled = scale_data(X_val_combined)\n",
    "X_test_scaled = scale_data(X_test_combined)\n",
    "\n",
    "print(\"Feature scaling complete.\")\n",
    "print(f\"  Train data shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Train data range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2120fc09",
   "metadata": {},
   "source": [
    "## 7. Prepare Dual Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd65ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: strong signal indicator (|heatmap| > 0.3)\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "\n",
    "y_train_class = (np.abs(y_train_combined) > CONFIDENCE_THRESHOLD).astype(np.float32)\n",
    "y_val_class = (np.abs(y_val_combined) > CONFIDENCE_THRESHOLD).astype(np.float32)\n",
    "y_test_class = (np.abs(y_test_combined) > CONFIDENCE_THRESHOLD).astype(np.float32)\n",
    "\n",
    "# Regression: signed heatmap\n",
    "y_train_reg = y_train_combined.astype(np.float32)\n",
    "y_val_reg = y_val_combined.astype(np.float32)\n",
    "y_test_reg = y_test_combined.astype(np.float32)\n",
    "\n",
    "print(f\"\\nDual targets prepared:\")\n",
    "print(f\"  Classification (good bets): {y_train_class.mean():.2%}\")\n",
    "print(f\"  Regression mean: {y_train_reg.mean():.4f}, std: {y_train_reg.std():.4f}\")\n",
    "print(f\"  Strong signals (|y| > 0.5): {np.sum(np.abs(y_train_reg) > 0.5)} / {len(y_train_reg)} ({np.mean(np.abs(y_train_reg) > 0.5):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb794ec",
   "metadata": {},
   "source": [
    "## 8. Define Model (Phase 5 Architecture with Increased Capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss from Phase 4/5\n",
    "class ConsistentPeakLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, target_height=0.7, peak_threshold=0.5, peak_weight=0.5,\n",
    "                 base_loss='mse', name=\"consistent_peak_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.target_height = target_height\n",
    "        self.peak_threshold = peak_threshold\n",
    "        self.peak_weight = peak_weight\n",
    "        self.base_loss = base_loss\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_pred = tf.reshape(y_pred, [-1])\n",
    "        \n",
    "        # Base loss\n",
    "        if self.base_loss == 'mse':\n",
    "            base_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        else:\n",
    "            delta = 1.0\n",
    "            abs_error = tf.abs(y_true - y_pred)\n",
    "            quadratic = tf.minimum(abs_error, delta)\n",
    "            linear = abs_error - quadratic\n",
    "            base_loss = tf.reduce_mean(0.5 * tf.square(quadratic) + delta * linear)\n",
    "        \n",
    "        # Peak penalty\n",
    "        strong_signal_mask = tf.cast(tf.abs(y_true) > self.peak_threshold, tf.float32)\n",
    "        target_values = tf.sign(y_true) * self.target_height * strong_signal_mask\n",
    "        signed_pred_magnitude = y_pred * tf.sign(y_true)\n",
    "        weak_prediction_mask = tf.cast(signed_pred_magnitude < self.target_height, tf.float32)\n",
    "        penalty_mask = strong_signal_mask * weak_prediction_mask\n",
    "        peak_mse = tf.square(y_pred - target_values) * penalty_mask\n",
    "        \n",
    "        num_penalty_points = tf.reduce_sum(penalty_mask)\n",
    "        peak_loss = tf.cond(\n",
    "            tf.greater(num_penalty_points, 0),\n",
    "            lambda: tf.reduce_sum(peak_mse) / num_penalty_points,\n",
    "            lambda: 0.0\n",
    "        )\n",
    "        \n",
    "        return base_loss + self.peak_weight * peak_loss\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'target_height': self.target_height,\n",
    "            'peak_threshold': self.peak_threshold,\n",
    "            'peak_weight': self.peak_weight,\n",
    "            'base_loss': self.base_loss\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def create_dual_branch_losses(classification_weight=0.4, regression_weight=0.6,\n",
    "                              peak_threshold=0.5, target_height=0.7, peak_weight=0.5):\n",
    "    classification_loss = 'binary_crossentropy'\n",
    "    regression_loss = ConsistentPeakLoss(\n",
    "        target_height=target_height,\n",
    "        peak_threshold=peak_threshold,\n",
    "        peak_weight=peak_weight,\n",
    "        base_loss='mse'\n",
    "    )\n",
    "    \n",
    "    loss_dict = {\n",
    "        'classification_output': classification_loss,\n",
    "        'regression_output': regression_loss\n",
    "    }\n",
    "    \n",
    "    loss_weights = {\n",
    "        'classification_output': classification_weight,\n",
    "        'regression_output': regression_weight\n",
    "    }\n",
    "    \n",
    "    return loss_dict, loss_weights\n",
    "\n",
    "print(\"Custom losses defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0cd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_asset_model(\n",
    "    input_shape: Tuple[int, int],\n",
    "    cnn_filters: List[int] = [32, 64, 128],  # Increased from Phase 5\n",
    "    lstm_units: int = 64,  # Increased from Phase 5\n",
    "    dense_units: List[int] = [64, 32],  # Increased from Phase 5\n",
    "    dropout_rate: float = 0.3,  # Reduced dropout (more data = less overfitting risk)\n",
    "    l2_reg: float = 0.005,  # Reduced L2 (more data)\n",
    "    name: str = \"multi_asset_dual_branch\"\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    Dual-branch model with increased capacity for multi-asset training.\n",
    "    \n",
    "    Key changes from Phase 5:\n",
    "    - More filters: 16→32→64 becomes 32→64→128\n",
    "    - Larger LSTM: 32 → 64 units\n",
    "    - Larger dense: [32,16] → [64,32]\n",
    "    - Less regularization (dropout 0.4 → 0.3, L2 0.01 → 0.005)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # === Shared CNN Feature Extractor ===\n",
    "    x = inputs\n",
    "    for i, filters in enumerate(cnn_filters):\n",
    "        x = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation=None,\n",
    "            kernel_regularizer=regularizers.l2(l2_reg),\n",
    "            name=f'conv1d_{i+1}'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(name=f'bn_conv_{i+1}')(x)\n",
    "        x = layers.Activation('relu', name=f'relu_conv_{i+1}')(x)\n",
    "        x = layers.Dropout(dropout_rate, name=f'dropout_conv_{i+1}')(x)\n",
    "    \n",
    "    shared_features = x\n",
    "    \n",
    "    # === Classification Branch ===\n",
    "    class_branch = layers.GlobalMaxPooling1D(name='class_pool')(shared_features)\n",
    "    for i, units in enumerate(dense_units):\n",
    "        class_branch = layers.Dense(\n",
    "            units,\n",
    "            activation=None,\n",
    "            kernel_regularizer=regularizers.l2(l2_reg),\n",
    "            name=f'class_dense_{i+1}'\n",
    "        )(class_branch)\n",
    "        class_branch = layers.BatchNormalization(name=f'bn_class_{i+1}')(class_branch)\n",
    "        class_branch = layers.Activation('relu', name=f'relu_class_{i+1}')(class_branch)\n",
    "        class_branch = layers.Dropout(dropout_rate, name=f'dropout_class_{i+1}')(class_branch)\n",
    "    \n",
    "    classification_output = layers.Dense(1, activation='sigmoid', name='classification_output')(class_branch)\n",
    "    \n",
    "    # === Regression Branch ===\n",
    "    reg_branch = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            lstm_units,\n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=regularizers.l2(l2_reg),\n",
    "            recurrent_regularizer=regularizers.l2(l2_reg),\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate * 0.5\n",
    "        ),\n",
    "        name='bilstm'\n",
    "    )(shared_features)\n",
    "    \n",
    "    for i, units in enumerate(dense_units):\n",
    "        reg_branch = layers.Dense(\n",
    "            units,\n",
    "            activation=None,\n",
    "            kernel_regularizer=regularizers.l2(l2_reg),\n",
    "            name=f'reg_dense_{i+1}'\n",
    "        )(reg_branch)\n",
    "        reg_branch = layers.BatchNormalization(name=f'bn_reg_{i+1}')(reg_branch)\n",
    "        reg_branch = layers.Activation('relu', name=f'relu_reg_{i+1}')(reg_branch)\n",
    "        reg_branch = layers.Dropout(dropout_rate, name=f'dropout_reg_{i+1}')(reg_branch)\n",
    "    \n",
    "    regression_output = layers.Dense(1, activation='tanh', name='regression_output')(reg_branch)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[classification_output, regression_output], name=name)\n",
    "    return model\n",
    "\n",
    "print(\"Multi-asset model architecture defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac2fb9",
   "metadata": {},
   "source": [
    "## 9. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'cnn_filters': [32, 64, 128],\n",
    "    'lstm_units': 64,\n",
    "    'dense_units': [64, 32],\n",
    "    'dropout_rate': 0.3,\n",
    "    'l2_reg': 0.005\n",
    "}\n",
    "\n",
    "# Loss configuration\n",
    "LOSS_CONFIG = {\n",
    "    'classification_weight': 0.4,\n",
    "    'regression_weight': 0.6,\n",
    "    'peak_threshold': 0.5,\n",
    "    'target_height': 0.7,\n",
    "    'peak_weight': 0.5\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 128,  # Larger batch for more data\n",
    "    'learning_rate': 0.001,\n",
    "    'patience': 10  # More patience with more data\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Model: {MODEL_CONFIG}\")\n",
    "print(f\"  Loss: {LOSS_CONFIG}\")\n",
    "print(f\"  Training: {TRAINING_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a3cab",
   "metadata": {},
   "source": [
    "## 10. Create and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c85e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "set_seed(42)\n",
    "model = create_multi_asset_model(\n",
    "    input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]),\n",
    "    **MODEL_CONFIG\n",
    ")\n",
    "\n",
    "# Compile\n",
    "loss_dict, loss_weights = create_dual_branch_losses(**LOSS_CONFIG)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=TRAINING_CONFIG['learning_rate']),\n",
    "    loss=loss_dict,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics={\n",
    "        'classification_output': ['accuracy', tf.keras.metrics.AUC(name='auc')],\n",
    "        'regression_output': ['mae', 'mse']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147921c",
   "metadata": {},
   "source": [
    "## 11. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9929d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = PHASE6_MODEL_DIR / 'best_model_multi_asset.keras'\n",
    "log_dir = PHASE6_MODEL_DIR / 'logs'\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "callbacks_list = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=TRAINING_CONFIG['patience'],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.CSVLogger(PHASE6_MODEL_DIR / 'training_history.csv')\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20d8f2",
   "metadata": {},
   "source": [
    "## 12. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20102eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    {\n",
    "        'classification_output': y_train_class,\n",
    "        'regression_output': y_train_reg\n",
    "    },\n",
    "    validation_data=(\n",
    "        X_val_scaled,\n",
    "        {\n",
    "            'classification_output': y_val_class,\n",
    "            'regression_output': y_val_reg\n",
    "        }\n",
    "    ),\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    epochs=TRAINING_CONFIG['epochs'],\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"  Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"  Final train_loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Final val_loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Overfitting gap: {history.history['val_loss'][-1] - history.history['loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33954758",
   "metadata": {},
   "source": [
    "## 13. Evaluate on Combined Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9706a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_class, y_pred_reg = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred_class = y_pred_class.flatten()\n",
    "y_pred_reg = y_pred_reg.flatten()\n",
    "y_pred_class_binary = (y_pred_class > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "test_metrics = {\n",
    "    'classification': {\n",
    "        'accuracy': float(accuracy_score(y_test_class, y_pred_class_binary)),\n",
    "        'precision': float(precision_score(y_test_class, y_pred_class_binary, zero_division=0)),\n",
    "        'recall': float(recall_score(y_test_class, y_pred_class_binary, zero_division=0)),\n",
    "        'f1_score': float(f1_score(y_test_class, y_pred_class_binary, zero_division=0))\n",
    "    },\n",
    "    'regression': {\n",
    "        'mae': float(mean_absolute_error(y_test_reg, y_pred_reg)),\n",
    "        'mse': float(mean_squared_error(y_test_reg, y_pred_reg)),\n",
    "        'rmse': float(np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))),\n",
    "        'r2': float(r2_score(y_test_reg, y_pred_reg)),\n",
    "        'direction_accuracy': float(np.mean(np.sign(y_test_reg) == np.sign(y_pred_reg)))\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 6 COMBINED TEST METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(test_metrics, indent=2))\n",
    "\n",
    "# Signal strength analysis\n",
    "strong_mask = np.abs(y_test_reg) > 0.5\n",
    "weak_mask = np.abs(y_test_reg) <= 0.5\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIGNAL STRENGTH ANALYSIS (Combined)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if strong_mask.sum() > 0:\n",
    "    mae_strong = mean_absolute_error(y_test_reg[strong_mask], y_pred_reg[strong_mask])\n",
    "    dir_acc_strong = np.mean(np.sign(y_test_reg[strong_mask]) == np.sign(y_pred_reg[strong_mask]))\n",
    "    avg_conf = np.abs(y_pred_reg[strong_mask]).mean()\n",
    "    \n",
    "    print(f\"\\nStrong Signals (|y| > 0.5): {strong_mask.sum()} samples\")\n",
    "    print(f\"  MAE: {mae_strong:.4f}\")\n",
    "    print(f\"  Direction Accuracy: {dir_acc_strong:.2%}\")\n",
    "    print(f\"  Avg Prediction Confidence: {avg_conf:.4f}\")\n",
    "\n",
    "if weak_mask.sum() > 0:\n",
    "    mae_weak = mean_absolute_error(y_test_reg[weak_mask], y_pred_reg[weak_mask])\n",
    "    dir_acc_weak = np.mean(np.sign(y_test_reg[weak_mask]) == np.sign(y_pred_reg[weak_mask]))\n",
    "    \n",
    "    print(f\"\\nWeak Signals (|y| ≤ 0.5): {weak_mask.sum()} samples\")\n",
    "    print(f\"  MAE: {mae_weak:.4f}\")\n",
    "    print(f\"  Direction Accuracy: {dir_acc_weak:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93580d8",
   "metadata": {},
   "source": [
    "## 14. Per-Asset Performance Analysis\n",
    "\n",
    "Test generalization across different assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_asset_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-ASSET PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for asset in splits_by_asset.keys():\n",
    "    # Get asset test data (already scaled in combined set)\n",
    "    asset_splits = splits_by_asset[asset]\n",
    "    X_test_asset = scale_data(asset_splits['X_test'])\n",
    "    y_test_asset = asset_splits['y_test']\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_class_asset, y_pred_reg_asset = model.predict(X_test_asset, verbose=0)\n",
    "    y_pred_reg_asset = y_pred_reg_asset.flatten()\n",
    "    \n",
    "    # Metrics\n",
    "    dir_acc = np.mean(np.sign(y_test_asset) == np.sign(y_pred_reg_asset))\n",
    "    mae = mean_absolute_error(y_test_asset, y_pred_reg_asset)\n",
    "    \n",
    "    # Strong signals\n",
    "    strong_mask_asset = np.abs(y_test_asset) > 0.5\n",
    "    if strong_mask_asset.sum() > 0:\n",
    "        dir_acc_strong_asset = np.mean(np.sign(y_test_asset[strong_mask_asset]) == np.sign(y_pred_reg_asset[strong_mask_asset]))\n",
    "        mae_strong_asset = mean_absolute_error(y_test_asset[strong_mask_asset], y_pred_reg_asset[strong_mask_asset])\n",
    "    else:\n",
    "        dir_acc_strong_asset = None\n",
    "        mae_strong_asset = None\n",
    "    \n",
    "    per_asset_results[asset] = {\n",
    "        'test_samples': len(y_test_asset),\n",
    "        'direction_accuracy': float(dir_acc),\n",
    "        'mae': float(mae),\n",
    "        'strong_signals': {\n",
    "            'count': int(strong_mask_asset.sum()),\n",
    "            'direction_accuracy': float(dir_acc_strong_asset) if dir_acc_strong_asset is not None else None,\n",
    "            'mae': float(mae_strong_asset) if mae_strong_asset is not None else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{asset}:\")\n",
    "    print(f\"  Test samples: {len(y_test_asset)}\")\n",
    "    print(f\"  Direction Accuracy: {dir_acc:.2%}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    if dir_acc_strong_asset is not None:\n",
    "        print(f\"  Strong Signal Dir Acc: {dir_acc_strong_asset:.2%} ({strong_mask_asset.sum()} samples)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a6461",
   "metadata": {},
   "source": [
    "## 15. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(PHASE6_MODEL_DIR / 'final_model_multi_asset.keras')\n",
    "print(f\"Model saved to {PHASE6_MODEL_DIR / 'final_model_multi_asset.keras'}\")\n",
    "\n",
    "# Save predictions\n",
    "np.save(PHASE6_MODEL_DIR / 'y_test_class_pred.npy', y_pred_class)\n",
    "np.save(PHASE6_MODEL_DIR / 'y_test_reg_pred.npy', y_pred_reg)\n",
    "print(\"Predictions saved.\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'model_config': MODEL_CONFIG,\n",
    "    'loss_config': LOSS_CONFIG,\n",
    "    'training_config': TRAINING_CONFIG,\n",
    "    'data_info': {\n",
    "        'assets': ASSETS,\n",
    "        'total_train_samples': len(X_train_combined),\n",
    "        'total_val_samples': len(X_val_combined),\n",
    "        'total_test_samples': len(X_test_combined),\n",
    "        'window_size': WINDOW_SIZE,\n",
    "        'n_features': X_train_scaled.shape[2]\n",
    "    },\n",
    "    'combined_test_metrics': test_metrics,\n",
    "    'per_asset_results': per_asset_results,\n",
    "    'signal_strength_analysis': {\n",
    "        'strong_signals': {\n",
    "            'count': int(strong_mask.sum()),\n",
    "            'mae': float(mae_strong) if strong_mask.sum() > 0 else None,\n",
    "            'direction_accuracy': float(dir_acc_strong) if strong_mask.sum() > 0 else None\n",
    "        },\n",
    "        'weak_signals': {\n",
    "            'count': int(weak_mask.sum()),\n",
    "            'mae': float(mae_weak) if weak_mask.sum() > 0 else None,\n",
    "            'direction_accuracy': float(dir_acc_weak) if weak_mask.sum() > 0 else None\n",
    "        }\n",
    "    },\n",
    "    'training_info': {\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'best_val_loss': float(min(history.history['val_loss'])),\n",
    "        'final_train_loss': float(history.history['loss'][-1]),\n",
    "        'final_val_loss': float(history.history['val_loss'][-1]),\n",
    "        'overfitting_gap': float(history.history['val_loss'][-1] - history.history['loss'][-1])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(PHASE6_MODEL_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {PHASE6_MODEL_DIR / 'results.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Phase 6 Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Achievements:\")\n",
    "print(f\"  ✓ Multi-asset training: {len(ASSETS)} FX pairs\")\n",
    "print(f\"  ✓ Total samples: {len(X_train_combined):,} (train)\")\n",
    "print(f\"  ✓ Increased model capacity: {MODEL_CONFIG}\")\n",
    "print(f\"  ✓ Asset-specific embeddings\")\n",
    "print(f\"  ✓ Per-asset performance analysis\")\n",
    "print(f\"\\n  Overfitting gap: {results['training_info']['overfitting_gap']:.4f}\")\n",
    "print(f\"  Strong signal direction accuracy: {dir_acc_strong:.2%}\" if strong_mask.sum() > 0 else \"  No strong signals in test\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  - Compare Phase 5 vs Phase 6 (single vs multi-asset)\")\n",
    "print(\"  - Backtest with meta-labeling\")\n",
    "print(\"  - Deploy for live trading\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
